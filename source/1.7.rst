千辛万苦训练好模型, 上线遇到问题? Paddle预测库轻松搞定模型部署
===============

0. 深度学习inference概述
------------

如果问在深度学习实践中，最难的部分是什么？猜测80%的开发者都会说：“当然是调参啊”。为什么难呢？因为调参就像厨师根据食材找到了料理配方，药剂师根据药材找到了药方，充满了玄幻色彩。小编却以为，掌握了调参，顶多算深度学习的绝学掌握了一半。“另一半是啥？”另一半就是“部署预测模型”。这有什么难的？好，我们打个比方，前面那位大厨会很多料理配方对吧。两个小时内，来一套满汉全席。“……”虽然比喻有些夸张，不过这也道出了深度学习模型训练和推理预测的关系。我们知道，深度学习一般分为训练和推理两个部分, 训练是神经网络"学习"的过程, 主要关注如何搜索和求解模型参数, 发现训练数据中的规律. 有了训练好的模型之后, 就要在在线环境中使用模型, 这个过程叫做推理, 推理过程中主要关注计算的性能. 

好了, 当我们千辛万苦训练好模型，终于要上线了，这个时候遇到了各种问题, 例如:

* 线上inference耗时太高, 直接造成服务不可用

* 模型上线后占用内存居高不下

* 在和训练不同的硬件环境中如何做推理


1. 飞桨推理引擎介绍
-----------

飞桨提供了这样一款性能强劲，上手简单的推理引擎，帮助用户摆脱各种上线的烦恼, 专注于搭建深度学习服务. 经过多个版本的升级迭代, 飞桨推理引擎已经具备了良好的用户体验, 包括了以下特性:

* 通用支持

Paddle Inference 是飞桨核心框架的推理引擎，它支持Server端X86 CPU，Nvidia GPU芯片，兼容Linux/Mac/Windows系统。同时Paddle Inference复用了飞桨训练前向代码, 支持所有飞桨训练产出模型。

* 高性能

Paddle Inference 针对不同平台不同的应用场景下进行了深度的优化, 保证预测期间的高吞吐，低时延。 

其中包括：   

1） 内存/显存复用     

在预测初始化阶段，飞桨预测引擎会对模型中的 OP 输出 Tensor 进行依赖分析，将两两互不依赖的 Tensor 在内存/显存空间上进行复用，进而增大模型的并行量，提升服务吞吐量，保证了任务的正常执行。

2） 支持细粒度OP横向纵向融合   

在预测初始化阶段，飞将预测引擎会按照已有的融合模式将模型中的多个Op融合成一个OP，减少了模型的计算量的同时，也减少了 Kernel Launch 的次数，从而能提升模型的性能。目前Paddle Inference中存在的融合模式多达几十个。
 

3） 高性能的CPU，GPU Kernel实现     

飞桨预测引擎内包含了同intel， NV共同打造的高性能的kernel，保证了模型高性能的执行。


4） 子图集成TensorRT   

PaddlePaddle采用子图的形式对TensorRT进行集成，当模型加载后，神经网络可以表示为由变量和运算节点组成的计算图。Paddle TensorRT实现的功能是对整个图进行扫描，发现图中可以使用TensorRT优化的子图，并使用TensorRT节点替换它们。在模型的推断期间，如果遇到TensorRT节点，Paddle会调用TensorRT库对该节点进行优化，其他的节点调用Paddle的原生实现。TensorRT在推断期间能够进行Op的横向和纵向融合，过滤掉冗余的Op，并对Op自动选择最优的kernel等进行优化，能够加快模型的预测速度。

5） 子图集成Paddle-Lite    

Paddle-Lite是飞桨团队开发的一款轻量级推理引擎, 飞桨推理引擎通过子图的方式集成了Paddle-Lite


6） 支持加载PaddleSlim模型预测   

* 易用性

易用性, 飞桨推理引擎完整支持C++, Python, C, Go和R的调用, 接口简单灵活，20行代码即可进行预测。


在深度学习训练过程中, Python是最常用的语言, 而在模型部署上线的时候, 不同的用户会使用不同的语言. 为了适应多种用户的需求, 目前飞桨推理引擎已经完整支持了C++, Python, C, Go和R, 所有这些语言底层都是通过C++实现的, 保证了性能的一致性,  对于这些语言之外的用户, 飞桨提供了ABI稳定的C API, 用户可以很方便地实现扩展.


* 多语言

在深度学习训练过程中, Python是最常用的语言, 而在模型部署上线的时候, 不同的用户会使用不同的语言. 为了适应多种用户的需求, 目前飞桨推理引擎已经完整支持了C++, Python, C, Go和R, 所有这些语言底层都是通过C++实现的, 保证了性能的一致性,  对于这些语言之外的用户, 飞桨提供了ABI稳定的C API, 用户可以很方便地实现扩展.


2. 使用示例
----------


2.1 模型保存
------------

首先, 我们需要训练并保存一个模型, Paddle提供了一个内置的函数 `save_inference_model`, 将训练模型保存为预测模型.

.. code:: python
    
    from paddle import fluid

    place = fluid.CPUPlace()
    executor = fluid.Executor(place)

    image = fluid.data(name="image", shape=[None, 28, 28], dtype="float32")
    label = fluid.data(name="label", shape=[None, 1], dtype="int64")

    feeder = fluid.DataFeeder(feed_list=[image, label], place=place)
    predict = fluid.layers.fc(input=image, size=10, act='softmax')

    loss = fluid.layers.cross_entropy(input=predict, label=label)
    avg_loss = fluid.layers.mean(loss)

    executor.run(fluid.default_startup_program())

    # 保存预测模型到model目录中, 只保存与输入image和输出predict相关的部分网络
    fluid.io.save_inference_model("model", feed_var_names=["image"],
        target_vars=[predict]. executor=executor)


.. tip::

    `save_inference_model`根据预测需要的输入和输出, 对训练模型进行剪枝, 去除和预测无关部分, 得到的模型相比训练更加精简, 适合优化和部署.


2.2 预测加载
-----------

有了预测模型之后, 就可以使用预测库了, Paddle提供了 AnalysisConfig 用于管理预测部署的各种设置, 用户可以根据自己的上线环境, 打开各种优化.

首先我们创建一个config

.. code:: python

    from paddle.fluid.core import AnalysisConfig

    # 创建配置对象
    config = AnalysisConfig("./model")



在Intel CPU上, 若硬件支持, 可以打开 `DNNL`_ (Deep Neural Network Library, 原名MKLDNN) 优化, 这是一个Intel开源的高性能计算库, 用于Intel架构的处理器和图形处理器上的深度学习优化, 飞桨推理引擎在后端将自动调用.

.. _DNNL: https://github.com/intel/mkl-dnn.git


.. code:: python

    config.enable_mkldnn()



对于需要使用Nvidia GPU用户, 只需要一行配置, 飞桨就会自动将计算切换到GPU上

.. code:: python

    # 在 GPU 0 上初始化 100 MB 显存。这只是一个初始值，实际显存可能会动态变化。
    config.enable_use_gpu(100, 0)


飞桨推理引擎提供了zero copy的方式管理输入和输出, 减少拷贝

.. code:: python

    # 打开zero copy
    config.switch_use_feed_fetch_ops(False)
    config.switch_specify_input_names(True)


设置好预测的配置后，就可以创建预测器了。


.. code:: python

    from paddle.fluid.core import create_paddle_predictor

    predictor = create_paddle_predictor(config)


.. tip::

    Paddle 预测提供了多项图优化，创建预测器时将会加载预测模型并自动进行图优化，以增强预测性能。


2.3 运行
------------

创建好predictor之后, 只需要传入数据就可以运行预测了, 这里假设我们已经将输入数据读入了一个numpy.ndarray数组中.


Paddle 提供了简单易用的API来管理输入和输出. 首先将输入数据传入predictor


.. code:: python

    input_names = predictor.get_input_names()
    # 得到输入 ZeroCopyTensor，前面保存的模型只有一个输入图片，多输入下的操作是类似的。
    input_tensor = predictor.get_input_tensor(input_names[0])

    input_tensor.copy_from_cpu(input_data.reshape([1, 28, 28]).astype("float32"))


运行推理引擎, 这里将会执行真正的计算


.. code:: python

    predictor.zero_copy_run()


解析结果到一个numpy数组中


.. code:: python

    ouput_names = predictor.get_output_names()
    # 获取输出 ZeroCopyTensor
    output_tensor = predictor.get_output_tensor(output_names[0])

    # 得到一个 numpy.ndarray 封装的输出数据
    output_data = output_tensor.copy_to_cpu()



2.4 性能优化
-------------

前面的内容已经介绍了飞桨推理引擎的使用方法, 但只具备基本的配置, 对于一些模型而言性能会有所不足，接下来让我们来熟悉进一步优化推理性能的方法吧。

NVIDIA TensorRT 是一个高性能的深度学习预测库，可为 GPU 上的深度学习推理应用程序提供低延迟和高吞吐量。
如果想要在 GPU 上进一步提高推理性能，可以尝试使用 Paddle-TensorRT。

Paddle 采用子图的形式对 TensorRT 进行了集成。使用 GPU 预测时，开启 TensorRT 在一些模型上可以提高性能。

在已经配置使用 GPU 预测的基础上, 只需要一行配置就可以开启 Paddle-TensorRT 加速预测：

.. code:: python

    config.enable_tensorrt_engine(workspace_size=1 << 30,
                                  max_batch_size=1,
                                  min_subgraph_size=3,  
                                  precision_mode=AnalysisConfig.Precision.Float32,
                                  use_static=False,
                                  use_calib_mode=False)

其中：

1. workspace_size 为 int 类型，指定TensorRT使用的工作空间大小，TensorRT会在该大小限制下筛选合适的kernel执行预测运算。建议设大一些，一般设为 1 << 20 至1 << 30；

2. max_batch_size 为 int 类型，指定最大的batch大小，运行时batch大小不得超过此限定值；

3. min_subgraph_size 为 int 类型，Paddle-TRT是以子图的形式运行，为了避免性能损失，当子图内部节点个数大于min_subgraph_size的时候，才会使用Paddle-TRT运行；

4. use_static 为 bool 类型，默认值为False。如果指定为True，在初次运行程序的时候会将TRT的优化信息进行序列化到磁盘上，下次运行时直接加载优化的序列化信息而不需要重新生成；

5. use_calib_mode 为 bool 类型，默认值为False。若要运行Paddle-TRT int8离线量化校准，需要将此选项设置为True。


TensorRT是NVIDIA的预测加速库，Paddle是怎样集成它的呢？其实，Paddle-TensorRT以子图的形式运行，当模型加载后，神经网络可以表示为由变量和运算节点组成的计算图。
Paddle-TensorRT实现的功能是对整个图进行扫描，发现图中可以使用TensorRT优化的子图，并使用TensorRT节点替换它们。在模型的推断期间，如果遇到TensorRT节点，
Paddle会调用TensorRT库对该节点进行优化，其他的节点调用Paddle的原生实现。TensorRT在推断期间能够进行Op的横向和纵向融合，过滤掉冗余的Op，
并对特定平台下的特定的Op选择合适的kernel等进行优化，能够加快模型的预测速度。


Paddle-Lite是飞桨团队开发的一款轻量级推理引擎, 飞桨推理引擎通过子图的方式集成了Paddle-Lite

Paddle Lite支持包括手机移动端在内的多种场景下的轻量、高效预测，支持广泛的硬件和平台，是一个高性能、轻量级的深度学习预测引擎。在保持和PaddlePaddle无缝对接外，也兼容支持其他训练框架产出的模型。Paddle Lite十分注重性能，从框架层面到底层算子都进行了全方位的优化，简要描述如下：

* 图分析优化
Lite 架构上有完整基于 C++ 开发的 IR 及相应 Pass 集合，以支持操作融合 (Operator fusion)，计算剪枝 (Computation pruning)，存储优化 (Memory optimization)，量化计算 (Quantitative computation) 等多类计算图优化。更多的优化策略可以简单通过添加 Pass 的方式模块化支持。

* Kernel优化
在 Kernel 层面，我们对相应硬件上的 Kernel 通过指令集、算法改写等方式进行了深度的优化。如在ARM上通过neno指令集实现了多个常用算子、在NVIDIA GPU上通过对算子的计算过程进行分析，改写算法，对多次的矩阵乘法进行融合等操作，提升kernel的运行效率。

* 量化支持
Lite 支持Paddle Slim 强大的量化训练完毕的模型，因此完整保留了量化计算的高性能以及量化训练的高精度。

* 框架执行
在框架执行方面，lite通过简化 Op 和 Kernel 的功能，使得执行期的框架开销极低；此外，框架极大的灵活性可以支持各种硬件的特定调度优化以提升整体效率。

* 多硬件混合调度
Lite 支持系统可见任意硬件的混合调度，目前已经支持 ARM CPU 和 ARM GPU 的 Kernel 自动混合调度，并验证了 X86 CPU 和 Nvidia GPU 间的混合调度。混合调度的支持，使得当系统内同时存在多种硬件可用时，Lite可以充分利用各类硬件资源，从而提升性能。
